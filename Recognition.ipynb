{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we are generating data for recogntion and training our recognition model\n",
    "\n",
    "## We are combining both synthtext data and icdar data for training recognition branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as Patches\n",
    "from shapely.geometry import Polygon\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import threading\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "import cv2\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as Patches\n",
    "from shapely.geometry import Polygon\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import scipy.io as sio\n",
    "try:\n",
    "    import queue\n",
    "except ImportError:\n",
    "    import Queue as queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofCCRnYMTtaO"
   },
   "outputs": [],
   "source": [
    "!unzip  \"/content/drive/MyDrive/SynthText.zip\" \n",
    "os.mkdir('synthtext')\n",
    "os.makedirs('synth/annotation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd4MzMkAUDpo"
   },
   "source": [
    "# Preparing the synthtext data from recogination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convrting synthtext dataset to cidar file format of ground truth\n",
    "\n",
    "# read data / path to gt.mat\n",
    "dataset = sio.loadmat('/content/drive/MyDrive/gt.mat')\n",
    "# 1. paths to imgs\n",
    "img_paths = [i[0] for i in dataset['imnames'][0, :]]\n",
    "with open('synthtext/path_to_imgs.txt', 'w') as f:\n",
    "    for item in img_paths:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# 2. word list\n",
    "dataset_word_list = []\n",
    "for img_words in dataset['txt'][0, :]:\n",
    "    img_word_list = []\n",
    "    for words in img_words:\n",
    "        for word in [item for sublist in [i.strip().split(' ') for i in words.split('\\n')] for item in sublist]:\n",
    "            if word != '':\n",
    "                img_word_list.append(word)\n",
    "    dataset_word_list.append(img_word_list)\n",
    "\n",
    "# 3. polys and words\n",
    "iter = 0\n",
    "for polys, words, img_path in tqdm(zip(dataset['wordBB'][0, :], dataset_word_list, img_paths)):\n",
    "\n",
    "    # fix poly\n",
    "    if len(polys.shape) == 2:\n",
    "        polys = polys[:, :, np.newaxis]\n",
    "    polys = polys.transpose([-1, 0, 1])\n",
    "\n",
    "    # check if all match up\n",
    "    if polys.shape[0] != len(words):\n",
    "        print('number of polys and words do not mathc')\n",
    "        break\n",
    "\n",
    "    # write file\n",
    "    with open('synth/annotation/{}.txt'.format(img_path.split('/')[1].split('.')[0]), 'w') as f:\n",
    "        for poly, word in zip(polys, words):\n",
    "            line = np.around(poly, 1).ravel()\n",
    "            line = np.concatenate([line, [word]], axis=0)\n",
    "            for item in line:\n",
    "                f.write(\"%s \" % item)\n",
    "            f.write(\"\\n\")\n",
    "    iter += 1\n",
    "    if iter % 100 == 0:\n",
    "        print(iter, 'out of', len(img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-x95WJtUjPe",
    "outputId": "8bc4a63e-b490-48e6-80eb-4c2ee34e6a16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 858750/858750 [06:46<00:00, 2110.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#vocabulary creation\n",
    "files=os.listdir('/content/synth/annotation')\n",
    "CLASSES=[]\n",
    "for i in tqdm(files):\n",
    "    file=open('/content/synth/annotation/'+i,'r')\n",
    "    for line in file:\n",
    "        line=line.replace('\\n','')\n",
    "        a=line.split(\",\")[-1]\n",
    "        if a!='':\n",
    "            CLASSES.append(a)\n",
    "NUM_CLASSES=len(list(set(CLASSES)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4L0Bx_YWY70U"
   },
   "outputs": [],
   "source": [
    "#All the functions in this block are used to generating ground truth values corresponding to image i,e score map ,geo_map and training mask\n",
    "\n",
    "def load_annotation(p):\n",
    "    '''\n",
    "    load polygon coordinate and text from the text file for corresponding image\n",
    "    here p is name of image file whose cooresponding annotation we want \n",
    "    '''\n",
    "    text_polys = []\n",
    "    text_tags = []\n",
    "    labels = []\n",
    "    \n",
    "    p=p.split('/')[-1]\n",
    "    p=p.replace('jpg','txt')\n",
    "    p=p.replace('png','txt')\n",
    "    p=p.replace('jpeg','txt')\n",
    "    p='synth/annotation/'+p\n",
    "    \n",
    "    if not os.path.exists(p):\n",
    "        return np.array(text_polys, dtype=np.float32)\n",
    "    with open(p, 'r', encoding='utf-8-sig') as f:\n",
    "        for line in f.readlines():\n",
    "            # strip BOM. \\ufeff for python3,  \\xef\\xbb\\bf for python2\n",
    "            # line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in line]\n",
    "            line = line.replace('\\xef\\xbb\\bf', '')\n",
    "            line = line.replace('\\xe2\\x80\\x8d', '')\n",
    "            line = line.strip()\n",
    "            line = line.split(' ')\n",
    "            if len(line) > 9:\n",
    "                label = line[8]\n",
    "                for i in range(len(line) - 9):\n",
    "                    label = label + \",\" + line[i + 9]\n",
    "            else:\n",
    "                label = line[-1]\n",
    "            # label = line[-1]\n",
    "            line = [line[0]] + [line[4]] + [line[1]] + [line[5]] + [line[2]] + [line[6]] + [line[3]] + [line[7]]\n",
    "            temp_line = map(eval, line[:8])\n",
    "            x1, y1, x2, y2, x3, y3, x4, y4 = map(float, temp_line)\n",
    "            # x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))\n",
    "            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n",
    "            if label == '*' or label == '###' or label == '':\n",
    "                text_tags.append(None)\n",
    "                #labels.append([-1])\n",
    "            else:\n",
    "                #labels.append(label_to_array(label))\n",
    "                text_tags.append(label)\n",
    "        return np.array(text_polys, dtype=np.float32), np.array(text_tags)\n",
    "\n",
    "# This Function is used to calculate AREA of polygon\n",
    "def polygon_area(poly):\n",
    "    '''\n",
    "    compute area of a polygon\n",
    "    '''\n",
    "    edge = [\n",
    "        (poly[1][0] - poly[0][0]) * (poly[1][1] + poly[0][1]),\n",
    "        (poly[2][0] - poly[1][0]) * (poly[2][1] + poly[1][1]),\n",
    "        (poly[3][0] - poly[2][0]) * (poly[3][1] + poly[2][1]),\n",
    "        (poly[0][0] - poly[3][0]) * (poly[0][1] + poly[3][1])\n",
    "    ]\n",
    "    return np.sum(edge)/2.\n",
    "\n",
    "# This function is used to discard invalid polygons and check direction of them\n",
    "\n",
    "def check_and_validate_polys(polys, tags, xxx_todo_changeme):\n",
    "\n",
    "\n",
    "    def is_polygon(poly):\n",
    "        for i in range(3):\n",
    "            p0 = poly[i]\n",
    "\n",
    "        p1 = poly[(i + 1) % 4]\n",
    "        p2 = poly[(i + 2) % 4]\n",
    "    \n",
    "        if p0[0] == p1[0] and p1[1] == p0[1]:\n",
    "            return False\n",
    "        if p0[0] == p2[0] and p2[1] == p0[1]:\n",
    "            return False\n",
    "        if p1[0] == p2[0] and p1[1] == p2[1]:\n",
    "            return False\n",
    "    \n",
    "        if p0[0] == p1[0]:\n",
    "            if p1[0] == p2[0]:\n",
    "                return False\n",
    "        else:\n",
    "            if p1[0] != p2[0]:\n",
    "                k1 = (p1[1] - p0[1]) / (p1[0] - p0[0])\n",
    "                k2 = (p2[1] - p1[1]) / (p2[0] - p1[0])\n",
    "                if abs(k1 - k2) < 1e-6:\n",
    "                    return False\n",
    "                else:\n",
    "                    if p1[1] == p2[1]:\n",
    "                        return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    (h, w) = xxx_todo_changeme\n",
    "    if polys.shape[0] == 0:\n",
    "        return polys\n",
    "    polys[:, :, 0] = np.clip(polys[:, :, 0], 0, w - 1)\n",
    "    polys[:, :, 1] = np.clip(polys[:, :, 1], 0, h - 1)\n",
    "\n",
    "    validated_polys = []\n",
    "    validated_tags = []\n",
    "    for poly, tag in zip(polys, tags):\n",
    "        p_area = polygon_area(poly)\n",
    "\n",
    "        # memory error after hitting not a polygon !!!\n",
    "        if is_polygon(poly) is False:\n",
    "            #print(\"not a polygon: \", poly)\n",
    "            continue\n",
    "\n",
    "        if abs(p_area) < 1:\n",
    "            # print poly\n",
    "            #print('invalid poly')\n",
    "            continue\n",
    "        if p_area > 0:\n",
    "            #print('poly in wrong direction')\n",
    "            poly = poly[(0, 3, 2, 1), :]\n",
    "        validated_polys.append(poly)\n",
    "        validated_tags.append(tag)\n",
    "    return np.array(validated_polys), np.array(validated_tags)    \n",
    "    \n",
    "    \n",
    "#This function is implementation of Polygon Shrinkage Algorithm \n",
    "def shrink_poly(poly, r):\n",
    "    '''\n",
    "    fit a poly inside the origin poly\n",
    "    used for generate the score map\n",
    "    '''\n",
    "    # shrink ratio\n",
    "    R = 0.3\n",
    "    # find the longer pair\n",
    "    if np.linalg.norm(poly[0] - poly[1]) + np.linalg.norm(poly[2] - poly[3]) > \\\n",
    "                    np.linalg.norm(poly[0] - poly[3]) + np.linalg.norm(poly[1] - poly[2]):\n",
    "        # first move (p0, p1), (p2, p3), then (p0, p3), (p1, p2)\n",
    "        ## p0, p1\n",
    "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
    "        poly[0][0] += R * r[0] * np.cos(theta)\n",
    "        poly[0][1] += R * r[0] * np.sin(theta)\n",
    "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
    "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
    "        ## p2, p3\n",
    "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
    "        poly[3][0] += R * r[3] * np.cos(theta)\n",
    "        poly[3][1] += R * r[3] * np.sin(theta)\n",
    "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
    "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
    "        ## p0, p3\n",
    "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
    "        poly[0][0] += R * r[0] * np.sin(theta)\n",
    "        poly[0][1] += R * r[0] * np.cos(theta)\n",
    "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
    "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
    "        ## p1, p2\n",
    "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
    "        poly[1][0] += R * r[1] * np.sin(theta)\n",
    "        poly[1][1] += R * r[1] * np.cos(theta)\n",
    "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
    "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
    "    else:\n",
    "        ## p0, p3\n",
    "        # print poly\n",
    "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
    "        poly[0][0] += R * r[0] * np.sin(theta)\n",
    "        poly[0][1] += R * r[0] * np.cos(theta)\n",
    "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
    "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
    "        ## p1, p2\n",
    "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
    "        poly[1][0] += R * r[1] * np.sin(theta)\n",
    "        poly[1][1] += R * r[1] * np.cos(theta)\n",
    "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
    "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
    "        ## p0, p1\n",
    "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
    "        poly[0][0] += R * r[0] * np.cos(theta)\n",
    "        poly[0][1] += R * r[0] * np.sin(theta)\n",
    "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
    "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
    "        ## p2, p3\n",
    "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
    "        poly[3][0] += R * r[3] * np.cos(theta)\n",
    "        poly[3][1] += R * r[3] * np.sin(theta)\n",
    "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
    "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
    "    return poly\n",
    "\n",
    "\n",
    "\n",
    "#Compute distance between p1-p2 and p3\n",
    "def point_dist_to_line(p1, p2, p3):\n",
    "    '''compute the distance from p3 to p1-p2'''\n",
    "    return np.linalg.norm(np.cross(p2 - p1, p1 - p3)) / np.linalg.norm(p2 - p1)\n",
    "\n",
    "#Find equation of line using two 2D points p1 and p2\n",
    "def fit_line(p1, p2):\n",
    "    '''fit a line ax+by+c = 0'''\n",
    "    if p1[0] == p1[1]:\n",
    "        return [1., 0., -p1[0]]\n",
    "    else:\n",
    "        [k, b] = np.polyfit(p1, p2, deg=1)\n",
    "        return [k, -1., b]\n",
    "\n",
    "#Find Intersection poitn of 2 lines\n",
    "def line_cross_point(line1, line2):\n",
    "    '''line1 0= ax+by+c, compute the cross point of line1 and line2'''\n",
    "    if line1[0] != 0 and line1[0] == line2[0]:\n",
    "        print('Cross point does not exist')\n",
    "        return None\n",
    "    if line1[0] == 0 and line2[0] == 0:\n",
    "        print('Cross point does not exist')\n",
    "        return None\n",
    "    if line1[1] == 0:\n",
    "        x = -line1[2]\n",
    "        y = line2[0] * x + line2[2]\n",
    "    elif line2[1] == 0:\n",
    "        x = -line2[2]\n",
    "        y = line1[0] * x + line1[2]\n",
    "    else:\n",
    "        k1, _, b1 = line1\n",
    "        k2, _, b2 = line2\n",
    "        x = -(b1-b2)/(k1-k2)\n",
    "        y = k1*x + b1\n",
    "    return np.array([x, y], dtype=np.float32)\n",
    "\n",
    "#Get Equation of line that is perpendicular to line passing through a point\n",
    "def line_verticle(line, point):\n",
    "    '''get the verticle line from line across point'''\n",
    "    if line[1] == 0:\n",
    "        verticle = [0, -1, point[1]]\n",
    "    else:\n",
    "        if line[0] == 0:\n",
    "            verticle = [1, 0, -point[0]]\n",
    "        else:\n",
    "            verticle = [-1./line[0], -1, point[1] - (-1/line[0] * point[0])]\n",
    "    return verticle\n",
    "\n",
    "# Convert a parallelogram to rectangle\n",
    "def rectangle_from_parallelogram(poly):\n",
    "    '''\n",
    "    fit a rectangle from a parallelogram\n",
    "    '''\n",
    "    p0, p1, p2, p3 = poly\n",
    "    angle_p0 = np.arccos(np.dot(p1-p0, p3-p0)/(np.linalg.norm(p0-p1) * np.linalg.norm(p3-p0)))\n",
    "    if angle_p0 < 0.5 * np.pi:\n",
    "        if np.linalg.norm(p0 - p1) > np.linalg.norm(p0-p3):\n",
    "            # p0 and p2\n",
    "            ## p0\n",
    "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
    "            p2p3_verticle = line_verticle(p2p3, p0)\n",
    "\n",
    "            new_p3 = line_cross_point(p2p3, p2p3_verticle)\n",
    "            ## p2\n",
    "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
    "            p0p1_verticle = line_verticle(p0p1, p2)\n",
    "\n",
    "            new_p1 = line_cross_point(p0p1, p0p1_verticle)\n",
    "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
    "        else:\n",
    "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
    "            p1p2_verticle = line_verticle(p1p2, p0)\n",
    "\n",
    "            new_p1 = line_cross_point(p1p2, p1p2_verticle)\n",
    "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
    "            p0p3_verticle = line_verticle(p0p3, p2)\n",
    "\n",
    "            new_p3 = line_cross_point(p0p3, p0p3_verticle)\n",
    "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
    "    else:\n",
    "        if np.linalg.norm(p0-p1) > np.linalg.norm(p0-p3):\n",
    "            # p1 and p3\n",
    "            ## p1\n",
    "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
    "            p2p3_verticle = line_verticle(p2p3, p1)\n",
    "\n",
    "            new_p2 = line_cross_point(p2p3, p2p3_verticle)\n",
    "            ## p3\n",
    "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
    "            p0p1_verticle = line_verticle(p0p1, p3)\n",
    "\n",
    "            new_p0 = line_cross_point(p0p1, p0p1_verticle)\n",
    "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n",
    "        else:\n",
    "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
    "            p0p3_verticle = line_verticle(p0p3, p1)\n",
    "\n",
    "            new_p0 = line_cross_point(p0p3, p0p3_verticle)\n",
    "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
    "            p1p2_verticle = line_verticle(p1p2, p3)\n",
    "\n",
    "            new_p2 = line_cross_point(p1p2, p1p2_verticle)\n",
    "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n",
    "\n",
    "#Sorting a rectangle to get all point in clockwies manner\n",
    "def sort_rectangle(poly):\n",
    "    '''sort the four coordinates of the polygon, points in poly should be sorted clockwise'''\n",
    "    # First find the lowest point\n",
    "    p_lowest = np.argmax(poly[:, 1])\n",
    "    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\n",
    "        # if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\n",
    "        p0_index = np.argmin(np.sum(poly, axis=1))\n",
    "        p1_index = (p0_index + 1) % 4\n",
    "        p2_index = (p0_index + 2) % 4\n",
    "        p3_index = (p0_index + 3) % 4\n",
    "        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\n",
    "    else:\n",
    "        # find the point that sits right to the lowest point\n",
    "        p_lowest_right = (p_lowest - 1) % 4\n",
    "        p_lowest_left = (p_lowest + 1) % 4\n",
    "        angle = np.arctan(-(poly[p_lowest][1] - poly[p_lowest_right][1])/(poly[p_lowest][0] - poly[p_lowest_right][0]))\n",
    "        # assert angle > 0\n",
    "        if angle <= 0:\n",
    "            print(angle, poly[p_lowest], poly[p_lowest_right])\n",
    "        if angle/np.pi * 180 > 45:\n",
    "            #this point is p2\n",
    "            p2_index = p_lowest\n",
    "            p1_index = (p2_index - 1) % 4\n",
    "            p0_index = (p2_index - 2) % 4\n",
    "            p3_index = (p2_index + 1) % 4\n",
    "            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi/2 - angle)\n",
    "        else:\n",
    "            # this point is p3\n",
    "            p3_index = p_lowest\n",
    "            p0_index = (p3_index + 1) % 4\n",
    "            p1_index = (p3_index + 2) % 4\n",
    "            p2_index = (p3_index + 3) % 4\n",
    "            return poly[[p0_index, p1_index, p2_index, p3_index]], angle\n",
    "\n",
    "\n",
    "def restore_rectangle_rbox(origin, geometry):\n",
    "    ''' Resotre rectangle tbox'''\n",
    "    d = geometry[:, :4]\n",
    "    angle = geometry[:, 4]\n",
    "    # for angle > 0\n",
    "    origin_0 = origin[angle >= 0]\n",
    "    d_0 = d[angle >= 0]\n",
    "    angle_0 = angle[angle >= 0]\n",
    "    if origin_0.shape[0] > 0:\n",
    "        p = np.array([np.zeros(d_0.shape[0]), -d_0[:, 0] - d_0[:, 2],\n",
    "                      d_0[:, 1] + d_0[:, 3], -d_0[:, 0] - d_0[:, 2],\n",
    "                      d_0[:, 1] + d_0[:, 3], np.zeros(d_0.shape[0]),\n",
    "                      np.zeros(d_0.shape[0]), np.zeros(d_0.shape[0]),\n",
    "                      d_0[:, 3], -d_0[:, 2]])\n",
    "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
    "\n",
    "        rotate_matrix_x = np.array([np.cos(angle_0), np.sin(angle_0)]).transpose((1, 0))\n",
    "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
    "\n",
    "        rotate_matrix_y = np.array([-np.sin(angle_0), np.cos(angle_0)]).transpose((1, 0))\n",
    "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
    "\n",
    "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "\n",
    "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
    "\n",
    "        p3_in_origin = origin_0 - p_rotate[:, 4, :]\n",
    "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
    "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
    "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
    "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
    "\n",
    "        new_p_0 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
    "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
    "    else:\n",
    "        new_p_0 = np.zeros((0, 4, 2))\n",
    "    # for angle < 0\n",
    "    origin_1 = origin[angle < 0]\n",
    "    d_1 = d[angle < 0]\n",
    "    angle_1 = angle[angle < 0]\n",
    "    if origin_1.shape[0] > 0:\n",
    "        p = np.array([-d_1[:, 1] - d_1[:, 3], -d_1[:, 0] - d_1[:, 2],\n",
    "                      np.zeros(d_1.shape[0]), -d_1[:, 0] - d_1[:, 2],\n",
    "                      np.zeros(d_1.shape[0]), np.zeros(d_1.shape[0]),\n",
    "                      -d_1[:, 1] - d_1[:, 3], np.zeros(d_1.shape[0]),\n",
    "                      -d_1[:, 1], -d_1[:, 2]])\n",
    "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
    "\n",
    "        rotate_matrix_x = np.array([np.cos(-angle_1), -np.sin(-angle_1)]).transpose((1, 0))\n",
    "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
    "\n",
    "        rotate_matrix_y = np.array([np.sin(-angle_1), np.cos(-angle_1)]).transpose((1, 0))\n",
    "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
    "\n",
    "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
    "\n",
    "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
    "\n",
    "        p3_in_origin = origin_1 - p_rotate[:, 4, :]\n",
    "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
    "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
    "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
    "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
    "\n",
    "        new_p_1 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
    "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
    "    else:\n",
    "        new_p_1 = np.zeros((0, 4, 2))\n",
    "    return np.concatenate([new_p_0, new_p_1])\n",
    "\n",
    "\n",
    "#Some geometrical functions used in codes\n",
    "def restore_rectangle(origin, geometry):\n",
    "    return restore_rectangle_rbox(origin, geometry)\n",
    "\n",
    "def getRotateRect(box):\n",
    "    rect = cv2.minAreaRect(box)\n",
    "\n",
    "    angle=rect[2]  # angle = [-90, 0)\n",
    "    if angle < -45:\n",
    "        rect = (rect[0], (rect[1][0], rect[1][1]), rect[2])\n",
    "        angle += 90\n",
    "        size = (rect[1][1],rect[1][0])\n",
    "    else:\n",
    "        rect = (rect[0], (rect[1][0], rect[1][1]), rect[2])\n",
    "        size=rect[1]\n",
    "\n",
    "    box_ = cv2.boxPoints(rect)\n",
    "    return np.concatenate([rect[0], size]), angle, box_\n",
    "\n",
    "\n",
    "#These Functions are used to Generate ROI params like out box,crop box & angles that we use to crop text from image\n",
    "def generate_roiRotatePara(box, angle, expand_w = 60):\n",
    "    '''Generate all ROI Parameterts'''\n",
    "    p0_rect, p1_rect, p2_rect, p3_rect = box\n",
    "    cxy = (p0_rect + p2_rect) / 2.\n",
    "    size = np.array([np.linalg.norm(p0_rect - p1_rect), np.linalg.norm(p0_rect - p3_rect)])\n",
    "    rrect = np.concatenate([cxy, size])\n",
    "\n",
    "    box=np.array(box)\n",
    "\n",
    "    points=np.array(box, dtype=np.int32)\n",
    "    xmin=np.min(points[:,0])\n",
    "    xmax=np.max(points[:,0])\n",
    "    ymin=np.min(points[:,1])\n",
    "    ymax=np.max(points[:,1])\n",
    "    bbox = np.array([xmin, ymin, xmax, ymax])\n",
    "    if np.any(bbox < -expand_w):\n",
    "        return None\n",
    "    \n",
    "    rrect[:2] -= bbox[:2]\n",
    "    rrect[:2] -= rrect[2:] / 2\n",
    "    rrect[2:] += rrect[:2]\n",
    "\n",
    "    bbox[2:] -= bbox[:2]\n",
    "\n",
    "    rrect[::2] = np.clip(rrect[::2], 0, bbox[2])\n",
    "    rrect[1::2] = np.clip(rrect[1::2], 0, bbox[3])\n",
    "    rrect[2:] -= rrect[:2]\n",
    "    \n",
    "    return bbox.astype(np.int32), rrect.astype(np.int32), - angle\n",
    "\n",
    "def restore_roiRotatePara(box):\n",
    "    rectange, rotate_angle = sort_rectangle(box)\n",
    "    return generate_roiRotatePara(rectange, rotate_angle)\n",
    "\n",
    "#This function is used to generate geo_map,score_map, training_mask,corp_box,out_box,angle that we use while training model\n",
    "def generate_rbox(im_size, polys, tags):\n",
    "    '''Genrate score_map and geo_map for image'''\n",
    "    h, w = im_size\n",
    "    poly_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    score_map = np.zeros((h, w), dtype=np.uint8)\n",
    "    geo_map = np.zeros((h, w, 5), dtype=np.float32)\n",
    "\n",
    "    outBoxs = []\n",
    "    cropBoxs = []\n",
    "    angles = []\n",
    "    text_tags = []\n",
    "    recg_masks = []\n",
    "    # mask used during traning, to ignore some hard areas\n",
    "    training_mask = np.ones((h, w), dtype=np.uint8)\n",
    "    for poly_idx, poly_tag in enumerate(zip(polys, tags)):\n",
    "        poly = poly_tag[0]\n",
    "        #print(poly)\n",
    "        tag = poly_tag[1]\n",
    "        #print(tag)\n",
    "        r = [None, None, None, None]\n",
    "        for i in range(4):\n",
    "            r[i] = min(np.linalg.norm(poly[i] - poly[(i + 1) % 4]),\n",
    "                       np.linalg.norm(poly[i] - poly[(i - 1) % 4]))\n",
    "        # score map\n",
    "        shrinked_poly = shrink_poly(poly.copy(), r).astype(np.int32)[np.newaxis, :, :]\n",
    "        cv2.fillPoly(score_map, shrinked_poly, 1)\n",
    "        cv2.fillPoly(poly_mask, shrinked_poly, poly_idx + 1)\n",
    "\n",
    "        # if geometry == 'RBOX':\n",
    "        # generate a parallelogram for any combination of two vertices\n",
    "        fitted_parallelograms = []\n",
    "        for i in range(4):\n",
    "            p0 = poly[i]\n",
    "            p1 = poly[(i + 1) % 4]\n",
    "            p2 = poly[(i + 2) % 4]\n",
    "            p3 = poly[(i + 3) % 4]\n",
    "            edge = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
    "            backward_edge = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
    "            forward_edge = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
    "            if point_dist_to_line(p0, p1, p2) > point_dist_to_line(p0, p1, p3):\n",
    "                #  parallel lines through p2\n",
    "                if edge[1] == 0:\n",
    "                    edge_opposite = [1, 0, -p2[0]]\n",
    "                else:\n",
    "                    edge_opposite = [edge[0], -1, p2[1] - edge[0] * p2[0]]\n",
    "            else:\n",
    "                # after p3\n",
    "                if edge[1] == 0:\n",
    "                    edge_opposite = [1, 0, -p3[0]]\n",
    "                else:\n",
    "                    edge_opposite = [edge[0], -1, p3[1] - edge[0] * p3[0]]\n",
    "            # move forward edge\n",
    "            new_p0 = p0\n",
    "            new_p1 = p1\n",
    "            new_p2 = p2\n",
    "            new_p3 = p3\n",
    "            new_p2 = line_cross_point(forward_edge, edge_opposite)\n",
    "            if point_dist_to_line(p1, new_p2, p0) > point_dist_to_line(p1, new_p2, p3):\n",
    "                # across p0\n",
    "                if forward_edge[1] == 0:\n",
    "                    forward_opposite = [1, 0, -p0[0]]\n",
    "                else:\n",
    "                    forward_opposite = [forward_edge[0], -1, p0[1] - forward_edge[0] * p0[0]]\n",
    "            else:\n",
    "                # across p3\n",
    "                if forward_edge[1] == 0:\n",
    "                    forward_opposite = [1, 0, -p3[0]]\n",
    "                else:\n",
    "                    forward_opposite = [forward_edge[0], -1, p3[1] - forward_edge[0] * p3[0]]\n",
    "            new_p0 = line_cross_point(forward_opposite, edge)\n",
    "            new_p3 = line_cross_point(forward_opposite, edge_opposite)\n",
    "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
    "            # or move backward edge\n",
    "            new_p0 = p0\n",
    "            new_p1 = p1\n",
    "            new_p2 = p2\n",
    "            new_p3 = p3\n",
    "            new_p3 = line_cross_point(backward_edge, edge_opposite)\n",
    "            if point_dist_to_line(p0, p3, p1) > point_dist_to_line(p0, p3, p2):\n",
    "                # across p1\n",
    "                if backward_edge[1] == 0:\n",
    "                    backward_opposite = [1, 0, -p1[0]]\n",
    "                else:\n",
    "                    backward_opposite = [backward_edge[0], -1, p1[1] - backward_edge[0] * p1[0]]\n",
    "            else:\n",
    "                # across p2\n",
    "                if backward_edge[1] == 0:\n",
    "                    backward_opposite = [1, 0, -p2[0]]\n",
    "                else:\n",
    "                    backward_opposite = [backward_edge[0], -1, p2[1] - backward_edge[0] * p2[0]]\n",
    "            new_p1 = line_cross_point(backward_opposite, edge)\n",
    "            new_p2 = line_cross_point(backward_opposite, edge_opposite)\n",
    "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
    "        areas = [Polygon(t).area for t in fitted_parallelograms]\n",
    "        parallelogram = np.array(fitted_parallelograms[np.argmin(areas)][:-1], dtype=np.float32)\n",
    "        # sort thie polygon\n",
    "        parallelogram_coord_sum = np.sum(parallelogram, axis=1)\n",
    "        min_coord_idx = np.argmin(parallelogram_coord_sum)\n",
    "        parallelogram = parallelogram[\n",
    "            [min_coord_idx, (min_coord_idx + 1) % 4, (min_coord_idx + 2) % 4, (min_coord_idx + 3) % 4]]\n",
    "\n",
    "        rectange = rectangle_from_parallelogram(parallelogram)\n",
    "        rectange, rotate_angle = sort_rectangle(rectange)\n",
    "\n",
    "        p0_rect, p1_rect, p2_rect, p3_rect = rectange\n",
    "\n",
    "        # if the poly is too small, then ignore it during training\n",
    "        poly_h = min(np.linalg.norm(p0_rect - p3_rect), np.linalg.norm(p1_rect - p2_rect))\n",
    "        poly_w = min(np.linalg.norm(p0_rect - p1_rect), np.linalg.norm(p2_rect - p3_rect))\n",
    "\n",
    "        invaild = (min(poly_h, poly_w) < 6) or tag is None or (True and poly_h > poly_w * 2)\n",
    "\n",
    "        if invaild:\n",
    "            cv2.fillPoly(training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
    "        xy_in_poly = np.argwhere(poly_mask == (poly_idx + 1))\n",
    "        \n",
    "        if not invaild:\n",
    "            roiRotatePara = generate_roiRotatePara(rectange, rotate_angle)\n",
    "            if roiRotatePara:\n",
    "                outBox, cropBox, angle = roiRotatePara\n",
    "                if min(cropBox[2:]) > 6:\n",
    "                    w , h = cropBox[2:]\n",
    "                    textImgW = np.ceil(min(w / float(h) * 32, 256) / 4 /1)\n",
    "                    #print(tag)\n",
    "                    if textImgW >= 2 * min(len(tag), 16):  # avoid CTC error\n",
    "                        outBoxs.append(outBox)\n",
    "                        cropBoxs.append(cropBox)\n",
    "                        angles.append(angle)\n",
    "                        text_tags.append(tag[:16])\n",
    "                        recg_masks.append(1.)\n",
    "\n",
    "        for y, x in xy_in_poly:\n",
    "            point = np.array([x, y], dtype=np.float32)\n",
    "            # top\n",
    "            geo_map[y, x, 0] = point_dist_to_line(p0_rect, p1_rect, point) + 3\n",
    "            # right\n",
    "            geo_map[y, x, 1] = point_dist_to_line(p1_rect, p2_rect, point) + 3\n",
    "            # down\n",
    "            geo_map[y, x, 2] = point_dist_to_line(p2_rect, p3_rect, point) + 3\n",
    "            # left\n",
    "            geo_map[y, x, 3] = point_dist_to_line(p3_rect, p0_rect, point) + 3\n",
    "            # angle\n",
    "            geo_map[y, x, 4] = rotate_angle\n",
    "    if len(outBoxs) == 0:\n",
    "        outBoxs.append([0, 0, 2 * 4, 2 * 4]) # keep extract From sharedConv feature map not zero\n",
    "        cropBoxs.append([0, 0, 2 * 4, 2 * 4])\n",
    "        angles.append(0.)\n",
    "        text_tags.append([NUM_CLASSES - 2])\n",
    "        recg_masks.append(0.)\n",
    "\n",
    "    outBoxs = np.array(outBoxs, np.int32)\n",
    "    cropBoxs = np.array(cropBoxs, np.int32)\n",
    "    angles = np.array(angles, np.float32)\n",
    "\n",
    "    return score_map, geo_map, training_mask, (outBoxs, cropBoxs, angles), text_tags, recg_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N_mGsD7qVRkY"
   },
   "outputs": [],
   "source": [
    "#This Function is used to prepare all images FRO icdar 2015 ORIGNAL Images\n",
    "def text_image_generation(input_size=512,random_scale=np.array([0.5, 3.0]),vis=False):\n",
    "    '''Genreating text Images From synthext  dataset'''\n",
    "\n",
    "    image_list=[]\n",
    "    for root, directories, files in os.walk('synthtext'):\n",
    "        for filename in files:\n",
    "            # join the two strings in order to form the full filepath.\n",
    "            if not filename.endswith('.txt'):\n",
    "                filepath = os.path.join(root, filename)\n",
    "                image_list.append(filepath)\n",
    "    #Taking sample of 5k images and generating text boxex \n",
    "    image_list5k=random.sample(image_list,5000)\n",
    "    index = np.arange(0, len(image_list5k))\n",
    "    np.random.shuffle(index)\n",
    "    c=0\n",
    "    paths=[]\n",
    "    words=[]\n",
    "    for i in tqdm(index):\n",
    "      try:\n",
    "        im_fn = image_list5k[i]\n",
    "        im = cv2.imread(im_fn,cv2.IMREAD_UNCHANGED)\n",
    "        h, w, _ = im.shape\n",
    "        txt_fn = im_fn\n",
    "        if not os.path.exists(txt_fn):\n",
    "          print('text file {} does not exists'.format(txt_fn))\n",
    "          continue\n",
    "            \n",
    "        text_polys, text_tags = load_annotation(txt_fn)\n",
    "        text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w))\n",
    "\n",
    "        \n",
    "        new_h, new_w, _ = im.shape\n",
    "        resize_h = new_h\n",
    "        resize_w = new_w\n",
    "       \n",
    "        score_map, geo_map, training_mask, rbox, text_tags, recg_mask = generate_rbox((new_h, new_w), text_polys, text_tags)\n",
    "          \n",
    "        outbox, cropbox,angle=rbox\n",
    "        for my in range(len(outbox)):\n",
    "          if(recg_mask[my]!=0):\n",
    "            out=outbox[my]\n",
    "            crop=cropbox[my]\n",
    "            if(im.shape[0]>out[3]+out[1] and im.shape[1]>out[2]+out[0] and out[2]>=0 and out[3]>=0 and out[1]>=0 and out[0]>=0):\n",
    "              ang=angle[my]\n",
    "              img1=tf.image.crop_to_bounding_box(im,out[1],out[0],out[3],out[2])\n",
    "              img2=tf.keras.preprocessing.image.random_rotation(img1,ang*180/np.pi,)\n",
    "              #img3=tf.image.crop_to_bounding_box(img2,crop[1],crop[0],crop[3],crop[2])\n",
    "              if not isinstance(img2,np.ndarray):\n",
    "                img2=img2.numpy()\n",
    "              \n",
    "              img3=cv2.resize(img2,(128,64),interpolation = cv2.INTER_AREA)\n",
    "              img3=cv2.detailEnhance(img3)\n",
    "              c+=1\n",
    "              cv2.imwrite('text_box_synth/word_'+str(c)+'.png',img3)\n",
    "              paths.append('text_box_synth/word_'+str(c)+'.png')\n",
    "              words.append(text_tags[my])\n",
    "              #print(c)         \n",
    "      except Exception as e:\n",
    "        print(image_list[i])\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "    data=pd.DataFrame({\"paths\":paths,\"words\":words})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRIAqPLOisOc",
    "outputId": "32ddbbd5-fa3d-4511-eaa5-3b2531c79df0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [2:08:11<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "data=text_image_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEK6FOL0jbFg"
   },
   "outputs": [],
   "source": [
    "#Saving the data\n",
    "data.to_csv('/content/drive/MyDrive/text_boxes/text_synthtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3XF8zRDZpPi"
   },
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "shutil.make_archive('text_boxes_synth', 'zip','/content/text_box_synth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkNop9vaO-B0",
    "outputId": "0e1b56ab-89a2-4ad3-81fc-69fa80133d70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39348"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.mkdir('text_box_synth')\n",
    "a=os.listdir('/content/text_box_synth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VASXIUbMs8h"
   },
   "outputs": [],
   "source": [
    "#loading the saved data\n",
    "!unzip /content/drive/MyDrive/text_boxes/text_box_synth.zip -d 'text_box_synth/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "YA4wds-NMtE-",
    "outputId": "0c6b1221-ced1-4953-c5ce-ec530090f989"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_box_synth/word_1.png</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_box_synth/word_2.png</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_box_synth/word_3.png</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_box_synth/word_4.png</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_box_synth/word_5.png</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       paths words\n",
       "0  text_box_synth/word_1.png   was\n",
       "1  text_box_synth/word_2.png  this\n",
       "2  text_box_synth/word_3.png  some\n",
       "3  text_box_synth/word_4.png    of\n",
       "4  text_box_synth/word_5.png   the"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_data=pd.read_csv('/content/drive/MyDrive/text_boxes/text_synthtext.csv',index_col=0)\n",
    "synth_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMgJRPQzWMYg"
   },
   "source": [
    "# preparing the icdar data for recoginition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the icdar word images data\n",
    "!!wget --header=\"Host: rrc.cvc.uab.es\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-GB;q=0.9,en-US;q=0.8,en;q=0.7,hi;q=0.6\" --header=\"Referer: https://rrc.cvc.uab.es/?ch=4&com=downloads\" --header=\"Cookie: _ga=GA1.2.567376463.1614948806; PHPSESSID=f2rmmqdv6bur83qvu12pg6bdip; _gid=GA1.2.798494208.1618176503\" --header=\"Connection: keep-alive\" \"https://rrc.cvc.uab.es/downloads/ch4_training_word_images_gt.zip\" -c -O 'ch4_training_word_images_gt.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yr2nySDpWpKG"
   },
   "outputs": [],
   "source": [
    "os.mkdir('icdar_text')\n",
    "!unzip \"/content/ch4_training_word_images_gt.zip\" -d \"/content/icdar_text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "LyQHzU2zWpVS",
    "outputId": "fc757638-872c-418c-cf09-a61c734e9133"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1_x</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>1_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word_1.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>\"Genaxis Theatre\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>word_2.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>\"[06]\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word_3.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>\"62-03\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word_4.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>\"Carpark\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word_5.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>\"EXIT\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0  1_x  2   3  4   5   6  7   8                 1_y\n",
       "0  word_1.png    0  0  86  0  88  13  1  13   \"Genaxis Theatre\"\n",
       "1  word_2.png    0  0  26  0  26  16  0  16              \"[06]\"\n",
       "2  word_3.png    0  0  59  0  59  19  0  19             \"62-03\"\n",
       "3  word_4.png    0  0  46  0  46  14  0  14           \"Carpark\"\n",
       "4  word_5.png    0  0  33  2  32  24  0  23              \"EXIT\""
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('/content/icdar_text/coords.txt',names=[0,1,2,3,4,5,6,7,8])\n",
    "df2=pd.read_csv('/content/icdar_text/gt.txt',names=[0,1])\n",
    "icdar_data=pd.merge(df1,df2,on=0)\n",
    "icdar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2-IZ73cAWpaA",
    "outputId": "e5289f5c-2bc9-44ea-c0bf-9a403fb46c2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>icdar_text/word_1.png</td>\n",
       "      <td>Genaxis Theatre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>icdar_text/word_2.png</td>\n",
       "      <td>[06]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>icdar_text/word_3.png</td>\n",
       "      <td>62-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>icdar_text/word_4.png</td>\n",
       "      <td>Carpark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>icdar_text/word_5.png</td>\n",
       "      <td>EXIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   paths            words\n",
       "0  icdar_text/word_1.png  Genaxis Theatre\n",
       "1  icdar_text/word_2.png             [06]\n",
       "2  icdar_text/word_3.png            62-03\n",
       "3  icdar_text/word_4.png          Carpark\n",
       "4  icdar_text/word_5.png             EXIT"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icdar_data=icdar_data.drop(columns=['1_x',2,3,4,5,6,7,8],axis=1)\n",
    "icdar_data=icdar_data.rename(columns={0: \"paths\", \"1_y\": \"words\"})\n",
    "icdar_data=icdar_data.replace(to_replace ='\"',value =\"\")\n",
    "icdar_data['words']=[i.strip().replace('\"','') for i in icdar_data['words']]\n",
    "icdar_data['paths']=[\"icdar_text/\"+i for i in icdar_data['paths'] ]\n",
    "icdar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "582XaYhKbaeF",
    "outputId": "a98f8a1c-0c2f-4d8d-ad7d-141620e232bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39348, 2)\n",
      "(4458, 2)\n"
     ]
    }
   ],
   "source": [
    "print(synth_data.shape)\n",
    "print(icdar_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhJ-64d3bT1x",
    "outputId": "6f754d7a-3ccf-4e75-d0ad-93b88978f781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43806, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data=pd.concat([synth_data,icdar_data])\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "AI-eYj4ucHXj",
    "outputId": "d6fe9301-213d-4860-e848-95d114a12fdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_box_synth/word_1.png</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_box_synth/word_2.png</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_box_synth/word_3.png</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_box_synth/word_4.png</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_box_synth/word_5.png</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       paths words\n",
       "0  text_box_synth/word_1.png   was\n",
       "1  text_box_synth/word_2.png  this\n",
       "2  text_box_synth/word_3.png  some\n",
       "3  text_box_synth/word_4.png    of\n",
       "4  text_box_synth/word_5.png   the"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "_4GJJPwkJs0i"
   },
   "outputs": [],
   "source": [
    "#Spliting the data into train and test\n",
    "train,test=train_test_split(final_data,test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xj0YbfzyKwKY"
   },
   "outputs": [],
   "source": [
    "#Preparing vocabulary for Text Recognition Branch\n",
    "CHAR_VECTOR = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZÉ´-~`<>'.:;^/|!?$%#@&*()[]{}_+=,\\\\\\\"\"\n",
    "NUM_CLASSES = len(CHAR_VECTOR) \n",
    "char_index={}\n",
    "index_char={}\n",
    "for i,val in enumerate(CHAR_VECTOR):\n",
    "  index_char[i+1]=val\n",
    "  char_index[val]=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KTY0j-_hK7od"
   },
   "outputs": [],
   "source": [
    "def recognizer_generator(batch_size,data):\n",
    "  '''Generator function for training in Text Recognition branch'''\n",
    "  df=data\n",
    "  while True:\n",
    "    try:\n",
    "      images=[]\n",
    "      vectors=[]\n",
    "      for ind in range(df.shape[0]):\n",
    "        path=df.iloc[ind]['paths']\n",
    "        img=cv2.imread(path)\n",
    "        img=cv2.detailEnhance(img)\n",
    "        img=cv2.resize(img, (128,64),cv2.INTER_NEAREST)\n",
    "        b=str(df.iloc[ind]['words'])\n",
    "        if len(b)==0:\n",
    "          continue\n",
    "        images.append(img)\n",
    "        vec=[]\n",
    "        for i1 in b:\n",
    "          vec.append(char_index[i1])\n",
    "        while(len(vec)<23):\n",
    "          vec.append(0)  \n",
    "        vec=np.array(vec)\n",
    "        vectors.append(vec)    \n",
    "        if len(vectors) == batch_size:\n",
    "          yield (np.array(images),np.array(vectors))\n",
    "          images=[]\n",
    "          vectors=[]\n",
    "        \n",
    "    except Exception as e:\n",
    "      import traceback\n",
    "      traceback.print_exc()\n",
    "      continue   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6qg7Pn2UgRs"
   },
   "source": [
    "# Building the recognition architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qlMcecn1LRFn"
   },
   "outputs": [],
   "source": [
    "#Text Recognition Model\n",
    "inputs = tf.keras.layers.Input(name='the_input', shape=(64,128,3), dtype='float32')  \n",
    "\n",
    "inner = tf.keras.layers.Conv2D(64, (3, 3), padding='same', name='conv1', kernel_initializer='he_normal')(inputs) \n",
    "inner = tf.keras.layers.BatchNormalization()(inner)\n",
    "inner = tf.keras.layers.Activation('relu')(inner)\n",
    "inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max1')(inner)\n",
    "\n",
    "#inner = tf.keras.layers.Dropout(0.2)(inner)\n",
    "\n",
    "inner = tf.keras.layers.Conv2D(64, (3, 3), padding='same', name='conv2', kernel_initializer='he_normal')(inner)  \n",
    "inner = tf.keras.layers.BatchNormalization()(inner)\n",
    "inner = tf.keras.layers.Activation('relu')(inner)\n",
    "inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max2')(inner)\n",
    "\n",
    "#inner = tf.keras.layers.Dropout(0.2)(inner)\n",
    "\n",
    "inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv3', kernel_initializer='he_normal')(inner)  \n",
    "inner = tf.keras.layers.BatchNormalization()(inner)\n",
    "inner = tf.keras.layers.Activation('relu')(inner)\n",
    "inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv4', kernel_initializer='he_normal')(inner)  \n",
    "inner = tf.keras.layers.BatchNormalization()(inner)\n",
    "inner = tf.keras.layers.Activation('relu')(inner)\n",
    "inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max3')(inner)  \n",
    "\n",
    "#inner = tf.keras.layers.Dropout(0.2)(inner)\n",
    "\n",
    "inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv5', kernel_initializer='he_normal')(inner)  \n",
    "inner = tf.keras.layers.BatchNormalization()(inner)\n",
    "inner = tf.keras.layers.Activation('relu')(inner)\n",
    "inner = tf.keras.layers.Conv2D(32, (3, 3), padding='same', name='conv6')(inner)   \n",
    "inner = tf.keras.layers.BatchNormalization()(inner)\n",
    "inner = tf.keras.layers.Activation('relu')(inner)\n",
    "inner = tf.keras.layers.MaxPooling2D(pool_size=(2, 1), name='max4')(inner)  \n",
    "\n",
    "#inner = tf.keras.layers.Dropout(0.2)(inner)\n",
    "\n",
    "inner = tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal', name='con7')(inner) \n",
    "inner = tf.keras.layers.BatchNormalization()(inner)\n",
    "inner = tf.keras.layers.Activation('relu')(inner)\n",
    "inner = tf.keras.layers.Reshape(target_shape=((64,512)), name='reshape')(inner)  \n",
    "inner = tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner) \n",
    "\n",
    "out=tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32,return_sequences=True,go_backwards=True))(inner)\n",
    "out=tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128,return_sequences=True,go_backwards=True))(out)\n",
    "x=tf.keras.layers.Dense(100)(out)#Here we hve given 100 because vocab size is 99 and 1 extra is for blank symbol\n",
    "x=tf.keras.activations.softmax(x)\n",
    "recognizer=tf.keras.models.Model(inputs,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QiwSCdxBLSTn"
   },
   "outputs": [],
   "source": [
    "# Note For Simplicity have used batch size of 128 while training Model\n",
    "#loss function for recognition \n",
    "def ctc_loss(y_true,y_pred):\n",
    "  #https://keras.io/examples/vision/captcha_ocr/\n",
    "  #https://stackoverflow.com/questions/64321779/how-to-use-tf-ctc-loss-with-variable-length-features-and-labels\n",
    "  label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True)\n",
    "  return tf.keras.backend.ctc_batch_cost(y_true,y_pred,np.ones((128,1),'int32')*64,label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UfJcNjnzLVbL"
   },
   "outputs": [],
   "source": [
    "recognizer.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01,amsgrad=True),loss=ctc_loss)\n",
    "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=2,mode='min',factor=0.75,verbose=1)\n",
    "early=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,mode='min',verbose=1)\n",
    "tensorboard=tf.keras.callbacks.TensorBoard('/text_boxes/recognizer',write_images=True,histogram_freq=1)  \n",
    "save=tf.keras.callbacks.ModelCheckpoint('/text_boxes/recognizer_icdar_best.h5',monitor='val_loss',mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ho-NiTmJLYXo",
    "outputId": "e896f3d8-a9d8-47fa-9c9f-91a31d98230a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "308/308 [==============================] - 130s 404ms/step - loss: 14.0182 - val_loss: 10.3180\n",
      "Epoch 2/20\n",
      "308/308 [==============================] - 122s 397ms/step - loss: 8.5666 - val_loss: 9.5709\n",
      "Epoch 3/20\n",
      "308/308 [==============================] - 122s 396ms/step - loss: 7.8286 - val_loss: 8.7111\n",
      "Epoch 4/20\n",
      "308/308 [==============================] - 122s 397ms/step - loss: 7.3591 - val_loss: 8.8563\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 5/20\n",
      "308/308 [==============================] - 121s 395ms/step - loss: 6.3358 - val_loss: 6.9304\n",
      "Epoch 6/20\n",
      "308/308 [==============================] - 121s 393ms/step - loss: 5.7233 - val_loss: 6.8472\n",
      "Epoch 7/20\n",
      "308/308 [==============================] - 121s 392ms/step - loss: 5.4600 - val_loss: 6.8264\n",
      "Epoch 8/20\n",
      "308/308 [==============================] - 120s 390ms/step - loss: 5.2607 - val_loss: 6.8324\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 9/20\n",
      "308/308 [==============================] - 120s 391ms/step - loss: 5.0647 - val_loss: 6.7473\n",
      "Epoch 10/20\n",
      "308/308 [==============================] - 121s 393ms/step - loss: 4.9852 - val_loss: 6.7422\n",
      "Epoch 11/20\n",
      "308/308 [==============================] - 121s 392ms/step - loss: 4.9486 - val_loss: 6.7415\n",
      "Epoch 12/20\n",
      "308/308 [==============================] - 121s 392ms/step - loss: 4.9186 - val_loss: 6.7423\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 13/20\n",
      "308/308 [==============================] - 123s 399ms/step - loss: 4.8932 - val_loss: 6.7280\n",
      "Epoch 14/20\n",
      "308/308 [==============================] - 124s 403ms/step - loss: 4.8824 - val_loss: 6.7236\n",
      "Epoch 15/20\n",
      "308/308 [==============================] - 123s 400ms/step - loss: 4.8759 - val_loss: 6.7217\n",
      "Epoch 16/20\n",
      "308/308 [==============================] - 122s 397ms/step - loss: 4.8711 - val_loss: 6.7209\n",
      "Epoch 17/20\n",
      "308/308 [==============================] - 122s 398ms/step - loss: 4.8669 - val_loss: 6.7205\n",
      "Epoch 18/20\n",
      "308/308 [==============================] - 122s 395ms/step - loss: 4.8632 - val_loss: 6.7202\n",
      "Epoch 19/20\n",
      "308/308 [==============================] - 122s 395ms/step - loss: 4.8598 - val_loss: 6.7202\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "Epoch 20/20\n",
      "308/308 [==============================] - 122s 395ms/step - loss: 4.8571 - val_loss: 6.7192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f21f61d3950>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer.fit(recognizer_generator(128,train),epochs=50,steps_per_epoch=train.shape[0]//128,\n",
    "               validation_data=recognizer_generator(128,test),validation_steps=test.shape[0]//128,callbacks=[reduce_lr,early,tensorborad,save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FdPZ3KlnIWM",
    "outputId": "97762d06-ac24-4a02-cfb3-733eeedbe5da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "308/308 [==============================] - 129s 399ms/step - loss: 7.4011 - val_loss: 8.4904\n",
      "Epoch 2/10\n",
      "308/308 [==============================] - 121s 394ms/step - loss: 7.0719 - val_loss: 8.5466\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.007499999832361937.\n",
      "Epoch 3/10\n",
      "308/308 [==============================] - 121s 395ms/step - loss: 6.5626 - val_loss: 7.8577\n",
      "Epoch 4/10\n",
      "308/308 [==============================] - 122s 396ms/step - loss: 6.2162 - val_loss: 8.1565\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.005624999874271452.\n",
      "Epoch 5/10\n",
      "308/308 [==============================] - 121s 394ms/step - loss: 5.8149 - val_loss: 7.3892\n",
      "Epoch 6/10\n",
      "308/308 [==============================] - 122s 396ms/step - loss: 5.5141 - val_loss: 7.4471\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.004218749818392098.\n",
      "Epoch 7/10\n",
      "308/308 [==============================] - 122s 396ms/step - loss: 5.2079 - val_loss: 7.5886\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.003164062276482582.\n",
      "Epoch 8/10\n",
      "308/308 [==============================] - 121s 393ms/step - loss: 4.8337 - val_loss: 7.5020\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0023730467073619366.\n",
      "Epoch 9/10\n",
      "308/308 [==============================] - 121s 394ms/step - loss: 4.4930 - val_loss: 7.2308\n",
      "Epoch 10/10\n",
      "308/308 [==============================] - 121s 394ms/step - loss: 4.1960 - val_loss: 7.3966\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0017797850305214524.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff0cfecfdd0>"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer.fit(recognizer_generator(128,train),epochs=10,steps_per_epoch=train.shape[0]//128,\n",
    "               validation_data=recognizer_generator(128,test),validation_steps=test.shape[0]//128,callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZuJ-xQ689En",
    "outputId": "d8547b0f-6dcc-4abb-cfca-c22b47bcc0b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "308/308 [==============================] - 121s 394ms/step - loss: 5.6790 - val_loss: 7.3445\n",
      "Epoch 2/5\n",
      "308/308 [==============================] - 122s 395ms/step - loss: 4.6763 - val_loss: 7.0657\n",
      "Epoch 3/5\n",
      "308/308 [==============================] - 122s 396ms/step - loss: 4.3792 - val_loss: 6.9320\n",
      "Epoch 4/5\n",
      "308/308 [==============================] - 122s 396ms/step - loss: 4.2034 - val_loss: 6.8541\n",
      "Epoch 5/5\n",
      "308/308 [==============================] - 122s 398ms/step - loss: 4.0816 - val_loss: 6.8031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff36c42c710>"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recognizer.fit(recognizer_generator(128,train),epochs=5,steps_per_epoch=train.shape[0]//128,\n",
    "               validation_data=recognizer_generator(128,test),validation_steps=test.shape[0]//128,callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "HANq8ESl5qP2"
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "recognizer.save_weights('/content/drive/MyDrive/text_boxes/recoginzer1.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fots.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
